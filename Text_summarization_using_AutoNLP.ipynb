{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text summarization using AutoNLP ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds-kdas/Hackathon---Club-Mahindra-customer-spend-prediction/blob/master/Text_summarization_using_AutoNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0pW32pH0O_x"
      },
      "source": [
        "**Configure git repo with username**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuHS25wSpxFz",
        "outputId": "b59f9901-bbe4-4c77-e0d1-311daf6e190b"
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 0s (19.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoKMBm8dqCNT"
      },
      "source": [
        "!git config --global user.email \"vimal.kumar477@gmail.com\"\n",
        "!git config --global user.name \"Vimal0307\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPd6QnAC0XUI"
      },
      "source": [
        "**Import required packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19gLsHFAlEf8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN1nyEuR0a3A"
      },
      "source": [
        "**Function to extract and map training and validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhrnNp-um4ol"
      },
      "source": [
        "data, info = tfds.load('cnn_dailymail', with_info=True)\n",
        "dataset_train, dataset_test = data['train'], data['test']\n",
        "def map_fn(x):\n",
        "    strings = [x['highlights']]\n",
        "    x['highlights'] = tf.strings.join(strings, separator=' ')\n",
        "    strings1 = [x['article']]\n",
        "    x['article'] = tf.strings.join(strings1, separator=' ')\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS2jCODD0jWJ"
      },
      "source": [
        "**Converting Tensorflow Dataset to CSV for AutoNLP- Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn6E2HDcqO23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0b9cb4-63cc-4935-929d-0a03d6664d2d"
      },
      "source": [
        "articles_train = []\n",
        "highlights_train = []\n",
        "dataset_train = dataset_train.map(map_fn)\n",
        "for i,j in zip(range(len(dataset_train)),tfds.as_numpy(dataset_train)):\n",
        "  if i ==0:\n",
        "      articles_train.append(str(j['article'][110:])) # Slice the text till 110th index as it containes unwanted text\n",
        "  else:\n",
        "    articles_train.append(str(j['article']))\n",
        "dataset_train = dataset_train.map(map_fn)\n",
        "for i,j in zip(range(len(dataset_train)),tfds.as_numpy(dataset_train)):\n",
        "  if i ==0:\n",
        "      highlights_train.append(str(j['highlights']))\n",
        "  else:\n",
        "    highlights_train.append(str(j['highlights']))\n",
        "print(len(highlights_train))\n",
        "#Create a data frame whyich can be converted as a CSV\n",
        "data = pd.DataFrame(\n",
        "    {'text': articles_train,\n",
        "     'target': highlights_train\n",
        "    })\n",
        "train_csv = data.to_csv(\"train_text.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "287113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RWCeHrP0xRg"
      },
      "source": [
        "**Converting Tensorflow Dataset to CSV for AutoNLP- Testing/Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE8JjAd7vbip",
        "outputId": "a842471f-cf0d-4ca9-b518-bb7a8a959f66"
      },
      "source": [
        "articles_test = []\n",
        "highlights_test = []\n",
        "dataset_test = dataset_test.map(map_fn)\n",
        "for i,j in zip(range(len(dataset_test)),tfds.as_numpy(dataset_test)):\n",
        "  if i ==0:\n",
        "      articles_test.append(str(j['article'][110:]))\n",
        "  else:\n",
        "    articles_test.append(str(j['article']))\n",
        "dataset_test = dataset_test.map(map_fn)\n",
        "for i,j in zip(range(len(dataset_test)),tfds.as_numpy(dataset_test)):\n",
        "  if i ==0:\n",
        "      highlights_test.append(str(j['highlights']))\n",
        "  else:\n",
        "    highlights_test.append(str(j['highlights']))\n",
        "print(len(highlights_test))\n",
        "data = pd.DataFrame(\n",
        "    {'text': articles_test,\n",
        "     'target': highlights_test\n",
        "    })\n",
        "test_csv = data.to_csv(\"test_text.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIgpoYJq00kI"
      },
      "source": [
        "**Install AutoNLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxLAyGHynOWC",
        "outputId": "14d6b197-99e6-4b12-d48d-744deedae8f0"
      },
      "source": [
        "!pip install -U autonlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autonlp\n",
            "  Downloading https://files.pythonhosted.org/packages/84/94/03e1e1fbe6fa0b4e494e1bc18b22a89af1df0d8bb8f2f733cc31c2a0808a/autonlp-0.2.6-py3-none-any.whl\n",
            "Collecting requests==2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 5.3MB/s \n",
            "\u001b[?25hCollecting tqdm==4.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.5MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting loguru==0.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.8MB/s \n",
            "\u001b[?25hCollecting prettytable==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/94/d5/52e48f3bcf66f838d411ad85c3ac9550c2451d082623e2d4d4df7411ed5c/prettytable-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->autonlp) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->autonlp) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->autonlp) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->autonlp) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->autonlp) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->autonlp) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from prettytable==2.0.0->autonlp) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable==2.0.0->autonlp) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub<0.1.0->autonlp) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub<0.1.0->autonlp) (3.7.4.3)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, tqdm, huggingface-hub, loguru, prettytable, autonlp\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: prettytable 2.1.0\n",
            "    Uninstalling prettytable-2.1.0:\n",
            "      Successfully uninstalled prettytable-2.1.0\n",
            "Successfully installed autonlp-0.2.6 huggingface-hub-0.0.8 loguru-0.5.3 prettytable-2.0.0 requests-2.25.1 tqdm-4.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDqwFBsk04sA"
      },
      "source": [
        "**Create account in hugging face and get API key from settings and login AutoNLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZxkVJTsnyeg",
        "outputId": "bf5fb622-3c5c-4a27-8392-e5f117eb1dab"
      },
      "source": [
        "!autonlp login --api-key <API KEY>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    üóù Successfully logged in as Vimal0703\u001b[0m\n",
            "> \u001b[1mINFO    üóù Storing credentials in:  /root/.autonlp\u001b[0m\n",
            "Welcome to ü§ó AutoNLP! Start by creating a project: \u001b[91mautonlp create_project\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRHsoI2n1Ev4"
      },
      "source": [
        "**Create a AutoNLP Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJRUxwO6pavf",
        "outputId": "c7d01ce8-4341-4c72-efe1-60173d80690a"
      },
      "source": [
        "! autonlp create_project --name summarization_model --language en --task summarization --max_models 5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    Creating project: summarization_model with task: summarization\u001b[0m\n",
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "> \u001b[1mINFO    ü§ô Project 'summarization_model' already exists, it was loaded successfully.\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "AutoNLP Project (id # 204)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            " ‚Ä¢ \u001b[1mName\u001b[0m:        \u001b[95msummarization_model\u001b[0m\n",
            " ‚Ä¢ \u001b[1mOwner\u001b[0m:       \u001b[92mVimal0703\u001b[0m\n",
            " ‚Ä¢ \u001b[1mStatus\u001b[0m:      \u001b[1m‚úÖ Successfully queued your models for training!\u001b[0m\n",
            " ‚Ä¢ \u001b[1mTask\u001b[0m:        \u001b[93mSummarization\u001b[0m\n",
            " ‚Ä¢ \u001b[1mCreated at\u001b[0m:  2021-05-19 12:18 Z\n",
            " ‚Ä¢ \u001b[1mLast update\u001b[0m: 2021-05-19 13:13 Z\n",
            "\n",
            "üí∞ Project current cost: \u001b[92mUSD 125.00\u001b[0m\n",
            "\n",
            "~~~~~~~~~~~~~~ \u001b[1mFiles\u001b[0m ~~~~~~~~~~~~~~\n",
            "\n",
            "Dataset ID:\n",
            "\u001b[96mVimal0703/autonlp-data-summarization_model\u001b[0m\n",
            "\n",
            "üìÅ \u001b[96mtrain_text.csv\u001b[0m (id # 253)\n",
            "   ‚Ä¢ \u001b[1mSplit\u001b[0m:             train\n",
            "   ‚Ä¢ \u001b[1mProcessing status\u001b[0m: ‚úÖ Success!\n",
            "   ‚Ä¢ \u001b[1mLast update\u001b[0m:       2021-05-19 12:48 Z\n",
            "üìÅ \u001b[96mtest_text.csv\u001b[0m (id # 254)\n",
            "   ‚Ä¢ \u001b[1mSplit\u001b[0m:             valid\n",
            "   ‚Ä¢ \u001b[1mProcessing status\u001b[0m: ‚úÖ Success!\n",
            "   ‚Ä¢ \u001b[1mLast update\u001b[0m:       2021-05-19 13:11 Z\n",
            "\n",
            "~~~~~~~~~~~~ \u001b[1mModels\u001b[0m ~~~~~~~~~~~\n",
            "\n",
            "+----+--------+----------------+--------------------+--------------------+\n",
            "|    |   ID   |     Status     |   Creation date    |    Last update     |\n",
            "+----+--------+----------------+--------------------+--------------------+\n",
            "| ‚úÖ | 204666 |    success     | 2021-05-19 13:13 Z | 2021-05-19 19:51 Z |\n",
            "| ‚úÖ | 204667 |    success     | 2021-05-19 13:13 Z | 2021-05-20 00:44 Z |\n",
            "| üèÉ | 204668 | model_training | 2021-05-19 13:13 Z | 2021-05-19 13:22 Z |\n",
            "| ‚úÖ | 204669 |    success     | 2021-05-19 13:13 Z | 2021-05-19 23:02 Z |\n",
            "| üèÉ | 204670 | model_training | 2021-05-19 13:13 Z | 2021-05-19 13:21 Z |\n",
            "+----+--------+----------------+--------------------+--------------------+\n",
            "Upload files to your project: \u001b[91mautonlp upload --project \"summarization_model\"\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvqqji81Iox"
      },
      "source": [
        "**Preparing Dataset for AutoNLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhFqRW9p1Ph4"
      },
      "source": [
        "Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rtdW7vWpisu",
        "outputId": "51857f01-717f-4c6e-ac2e-9e48591d205c"
      },
      "source": [
        "!autonlp upload --project summarization_model --split train \\\n",
        "            --col_mapping text:text,target:target \\\n",
        "            --files '/content/train_text.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    Uploading files for project: summarization_model\u001b[0m\n",
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Retrieving project 'summarization_model' from AutoNLP...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully loaded project: 'summarization_model'!\u001b[0m\n",
            "> \u001b[1mINFO    Mapping: {'text': 'text', 'target': 'target'}\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üîé Validating /content/train_text.csv and column mapping...\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üì¶ Copying /content/train_text.csv to /root/.huggingface/autonlp/projects/Vimal0703/autonlp-data-summarization_model/raw/train_text.csv...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Uploading files to the dataset hub...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully uploaded  the files!\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üìÅ Registering file train_text.csv into project 'train_text.csv'...\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] ‚úÖ Success!\u001b[0m\n",
            "üéâ Yupee! Your files have been uploaded.\n",
            "Once you're done, starting a training here: \u001b[91mautonlp train --project summarization_model\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DupnukEQ1SdI"
      },
      "source": [
        "Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYWdlC-pwAwh",
        "outputId": "9ab63c4a-daa2-42af-9535-92f89e07031b"
      },
      "source": [
        "!autonlp upload --project summarization_model --split valid \\\n",
        "            --col_mapping text:text,target:target \\\n",
        "            --files '/content/test_text.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    Uploading files for project: summarization_model\u001b[0m\n",
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Retrieving project 'summarization_model' from AutoNLP...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully loaded project: 'summarization_model'!\u001b[0m\n",
            "> \u001b[1mINFO    Mapping: {'text': 'text', 'target': 'target'}\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üîé Validating /content/test_text.csv and column mapping...\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üì¶ Copying /content/test_text.csv to /root/.huggingface/autonlp/projects/Vimal0703/autonlp-data-summarization_model/raw/test_text.csv...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Uploading files to the dataset hub...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully uploaded  the files!\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] üìÅ Registering file test_text.csv into project 'test_text.csv'...\u001b[0m\n",
            "> \u001b[1mINFO    [1/1] ‚úÖ Success!\u001b[0m\n",
            "üéâ Yupee! Your files have been uploaded.\n",
            "Once you're done, starting a training here: \u001b[91mautonlp train --project summarization_model\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jehe2f-N1Udg"
      },
      "source": [
        "**Train AutoNLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SseW2jUNvKhJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGjmrwnqrDmr",
        "outputId": "f348318c-2e6b-4163-ea26-413d377f9661"
      },
      "source": [
        "!autonlp train --project summarization_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    Starting Training For Project: summarization_model\u001b[0m\n",
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Retrieving project 'summarization_model' from AutoNLP...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully loaded project: 'summarization_model'!\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "> \u001b[1mINFO    üîé Calculating a cost estimate for the training...\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 964, in communicate\n",
            "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1715, in _communicate\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/autonlp\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/autonlp/cli/autonlp.py\", line 52, in main\n",
            "    command.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/autonlp/cli/train.py\", line 39, in run\n",
            "    project.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/autonlp/project.py\", line 316, in train\n",
            "    dataset_repo = self._clone_dataset_repo()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/autonlp/project.py\", line 379, in _clone_dataset_repo\n",
            "    dataset_repo.git_pull()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/huggingface_hub/repository.py\", line 355, in git_pull\n",
            "    cwd=self.local_dir,\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 975, in communicate\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWfvzE9g1YTI"
      },
      "source": [
        "**Monitor Training progress**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jqsBlPWwPPJ",
        "outputId": "dcddebba-ba2c-433b-9b88-7089ca080d30"
      },
      "source": [
        "! autonlp project_info --name summarization_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    Fetching info for project: summarization_model\u001b[0m\n",
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "> \u001b[1mINFO    ‚òÅ Retrieving project 'summarization_model' from AutoNLP...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing uploaded files information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing models information...\u001b[0m\n",
            "> \u001b[1mINFO    üîÑ Refreshing cost information...\u001b[0m\n",
            "> \u001b[1mINFO    ‚úÖ Successfully loaded project: 'summarization_model'!\u001b[0m\n",
            "AutoNLP Project (id # 204)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            " ‚Ä¢ \u001b[1mName\u001b[0m:        \u001b[95msummarization_model\u001b[0m\n",
            " ‚Ä¢ \u001b[1mOwner\u001b[0m:       \u001b[92mVimal0703\u001b[0m\n",
            " ‚Ä¢ \u001b[1mStatus\u001b[0m:      \u001b[1m‚úÖ Successfully queued your models for training!\u001b[0m\n",
            " ‚Ä¢ \u001b[1mTask\u001b[0m:        \u001b[93mSummarization\u001b[0m\n",
            " ‚Ä¢ \u001b[1mCreated at\u001b[0m:  2021-05-19 12:18 Z\n",
            " ‚Ä¢ \u001b[1mLast update\u001b[0m: 2021-05-19 13:13 Z\n",
            "\n",
            "üí∞ Project current cost: \u001b[92mUSD 125.00\u001b[0m\n",
            "\n",
            "~~~~~~~~~~~~~~ \u001b[1mFiles\u001b[0m ~~~~~~~~~~~~~~\n",
            "\n",
            "Dataset ID:\n",
            "\u001b[96mVimal0703/autonlp-data-summarization_model\u001b[0m\n",
            "\n",
            "üìÅ \u001b[96mtrain_text.csv\u001b[0m (id # 253)\n",
            "   ‚Ä¢ \u001b[1mSplit\u001b[0m:             train\n",
            "   ‚Ä¢ \u001b[1mProcessing status\u001b[0m: ‚úÖ Success!\n",
            "   ‚Ä¢ \u001b[1mLast update\u001b[0m:       2021-05-19 12:48 Z\n",
            "üìÅ \u001b[96mtest_text.csv\u001b[0m (id # 254)\n",
            "   ‚Ä¢ \u001b[1mSplit\u001b[0m:             valid\n",
            "   ‚Ä¢ \u001b[1mProcessing status\u001b[0m: ‚úÖ Success!\n",
            "   ‚Ä¢ \u001b[1mLast update\u001b[0m:       2021-05-19 13:11 Z\n",
            "\n",
            "~~~~~~~~~~~~ \u001b[1mModels\u001b[0m ~~~~~~~~~~~\n",
            "\n",
            "+----+--------+----------------+--------------------+--------------------+\n",
            "|    |   ID   |     Status     |   Creation date    |    Last update     |\n",
            "+----+--------+----------------+--------------------+--------------------+\n",
            "| ‚úÖ | 204666 |    success     | 2021-05-19 13:13 Z | 2021-05-19 19:51 Z |\n",
            "| ‚úÖ | 204667 |    success     | 2021-05-19 13:13 Z | 2021-05-20 00:44 Z |\n",
            "| üèÉ | 204668 | model_training | 2021-05-19 13:13 Z | 2021-05-19 13:22 Z |\n",
            "| ‚úÖ | 204669 |    success     | 2021-05-19 13:13 Z | 2021-05-19 23:02 Z |\n",
            "| üèÉ | 204670 | model_training | 2021-05-19 13:13 Z | 2021-05-19 13:21 Z |\n",
            "+----+--------+----------------+--------------------+--------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDagRfReqn3N",
        "outputId": "85ea3b48-a024-4ac6-ad27-89a87cd7ab97"
      },
      "source": [
        "!autonlp predict --project summarization_model --model_id 204667 --sentence \"At the beginning of the 21st century China had no high-speed railways.Slow and often uncomfortable trains plodded across this vast country, with low average speeds making journeys such as Shanghai-Beijing a test of endurance.Today, it's a completely different picture. The world's most populous nation has -- by some distance -- the world's largest network of high-speed railways.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[1mINFO    üóù Retrieving credentials from config...\u001b[0m\n",
            "{'error': 'Model Vimal0703/autonlp-summarization_model-204667 is currently loading', 'estimated_time': 35.669496}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrbQgQ2EcUd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}